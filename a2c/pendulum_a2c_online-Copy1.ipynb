{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, input): return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert numpy arrays to tensors\n",
    "def t(x):\n",
    "    x = np.array(x) if not isinstance(x, np.ndarray) else x\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, n_actions)\n",
    "        )\n",
    "        \n",
    "        logstds_param = nn.Parameter(torch.full((n_actions,), 0.1))\n",
    "        self.register_parameter(\"logstds\", logstds_param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        means = self.model(X)\n",
    "        stds = torch.clamp(self.logstds.exp(), 1e-3, 50)\n",
    "        \n",
    "        return torch.distributions.Normal(means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Critic module\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 64),\n",
    "            activation(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v0\")\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, n_actions, activation=Mish)\n",
    "critic = Critic(state_dim, activation=Mish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(rewards, dones, gamma):\n",
    "    ret = 0\n",
    "    discounted = []\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        ret = reward + ret * gamma * (1-done)\n",
    "        discounted.append(ret)\n",
    "    \n",
    "    return discounted[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_memory(memory, last_value, discount_rewards=True):\n",
    "    actions = []\n",
    "    states = []\n",
    "    next_states = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "\n",
    "    for action, reward, state, next_state, done in memory:\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "    \n",
    "    if discount_rewards:\n",
    "        if dones[-1] == 0:\n",
    "            rewards = discounted_rewards(rewards + [last_value], dones + [0], gamma)[:-1]\n",
    "        else:\n",
    "            rewards = discounted_rewards(rewards, dones, gamma)\n",
    "\n",
    "    actions = t(actions).view(-1, 1)\n",
    "    states = t(states)\n",
    "    next_states = t(next_states)\n",
    "    rewards = t(rewards).view(-1, 1)\n",
    "    dones = t(dones).view(-1, 1)\n",
    "    return actions, rewards, states, next_states, dones\n",
    "\n",
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CLearner():\n",
    "    def __init__(self, env, actor, critic, gamma=0.9, entropy_beta=0,\n",
    "                 actor_lr=4e-4, critic_lr=4e-3, max_grad_norm=0.5):\n",
    "        self.gamma = gamma\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.actor_optim = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = torch.optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def learn(self, memory, last_value, steps, discount_rewards=True):\n",
    "        actions, rewards, states, next_states, dones = process_memory(memory, last_value, discount_rewards)\n",
    "\n",
    "        if discount_rewards:\n",
    "            td_target = rewards\n",
    "        else:\n",
    "            td_target = rewards + gamma*critic(next_states)*(1-dones)\n",
    "        value = critic(states)\n",
    "        advantage = td_target - value\n",
    "\n",
    "        # actor\n",
    "        norm_dists = self.actor(states)\n",
    "        logs_probs = norm_dists.log_prob(actions)\n",
    "        entropy = norm_dists.entropy().mean()\n",
    "\n",
    "        actor_loss = (-logs_probs*advantage.detach()).mean() - entropy*entropy_beta\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        clip_grad_norm_(self.actor_optim, self.max_grad_norm)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # critic\n",
    "        critic_loss = F.mse_loss(td_target, value)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        clip_grad_norm_(self.critic_optim, self.max_grad_norm)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # reports\n",
    "        writer.add_scalar(\"losses/log_probs\", -logs_probs.mean(), global_step=steps)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy, global_step=steps) \n",
    "        writer.add_scalar(\"losses/entropy_beta\", entropy_beta, global_step=steps) \n",
    "        writer.add_scalar(\"losses/actor\", actor_loss, global_step=steps)\n",
    "        writer.add_scalar(\"losses/advantage\", advantage.mean(), global_step=steps)\n",
    "        writer.add_scalar(\"losses/critic\", critic_loss, global_step=steps)\n",
    "\n",
    "learner = A2CLearner(env, actor, critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 , episode reward: -1180.6139006557023\n",
      "episode: 10 , episode reward: -1252.2319197901786\n",
      "episode: 20 , episode reward: -1365.4321144410337\n",
      "episode: 30 , episode reward: -1169.5860742509194\n",
      "episode: 40 , episode reward: -1552.9231831087127\n",
      "episode: 50 , episode reward: -980.5214001353957\n",
      "episode: 60 , episode reward: -983.9049866974093\n",
      "episode: 70 , episode reward: -1178.8119621594988\n",
      "episode: 80 , episode reward: -1276.2545393392404\n",
      "episode: 90 , episode reward: -1007.3665244333504\n",
      "episode: 100 , episode reward: -1642.8551018817825\n",
      "episode: 110 , episode reward: -1385.0084370249972\n",
      "episode: 120 , episode reward: -717.6605853767742\n",
      "episode: 130 , episode reward: -726.2617473177773\n",
      "episode: 140 , episode reward: -712.6438170658791\n",
      "episode: 150 , episode reward: -409.4135211926305\n",
      "episode: 160 , episode reward: -545.0475985641392\n",
      "episode: 170 , episode reward: -542.3393151405004\n",
      "episode: 180 , episode reward: -536.5122622360361\n",
      "episode: 190 , episode reward: -806.3975705036598\n",
      "episode: 200 , episode reward: -1406.719524113011\n",
      "episode: 210 , episode reward: -890.1904274642486\n",
      "episode: 220 , episode reward: -703.5593381509868\n",
      "episode: 230 , episode reward: -566.4663835615551\n",
      "episode: 240 , episode reward: -427.6649635029781\n",
      "episode: 250 , episode reward: -3.400294123793768\n",
      "episode: 260 , episode reward: -1014.3203503740777\n",
      "episode: 270 , episode reward: -667.254322816758\n",
      "episode: 280 , episode reward: -4.375583022840747\n",
      "episode: 290 , episode reward: -1391.747502981041\n",
      "episode: 300 , episode reward: -279.4095557064167\n",
      "episode: 310 , episode reward: -1360.859802865417\n",
      "episode: 320 , episode reward: -268.321783690708\n",
      "episode: 330 , episode reward: -268.1607000475911\n",
      "episode: 340 , episode reward: -431.72808172218936\n",
      "episode: 350 , episode reward: -603.3310402283485\n",
      "episode: 360 , episode reward: -130.1518446000897\n",
      "episode: 370 , episode reward: -1050.3900137422115\n",
      "episode: 380 , episode reward: -1261.400434143454\n",
      "episode: 390 , episode reward: -134.34658042843506\n",
      "episode: 400 , episode reward: -130.06265456523727\n",
      "episode: 410 , episode reward: -434.73749876527614\n",
      "episode: 420 , episode reward: -428.59714101432684\n",
      "episode: 430 , episode reward: -1219.6909273551496\n",
      "episode: 440 , episode reward: -264.8045939142887\n",
      "episode: 450 , episode reward: -139.51256679583696\n",
      "episode: 460 , episode reward: -132.21161723770302\n",
      "episode: 470 , episode reward: -141.13445919352094\n",
      "episode: 480 , episode reward: -726.4931511450974\n",
      "episode: 490 , episode reward: -1.9454868010616058\n",
      "episode: 500 , episode reward: -1.4788592021667921\n",
      "episode: 510 , episode reward: -130.95363278615903\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-414-59c0ad1ce8a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mlast_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-413-e120d8acb485>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, memory, last_value, steps, discount_rewards)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mcritic_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcritic_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.asdf/installs/python/3.6.3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.asdf/installs/python/3.6.3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_rewards = []\n",
    "total_steps = 0\n",
    "memory = []\n",
    "\n",
    "for i in range(1000):\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    max_steps = 16\n",
    "    steps = 0\n",
    "    memory.clear()\n",
    "\n",
    "    while not done:\n",
    "        dists = actor(t(state))\n",
    "        actions = dists.sample()\n",
    "        actions_clamped = torch.clamp(actions, env.action_space.low.min(), env.action_space.high.max())\n",
    "        \n",
    "        next_state, reward, done, info = env.step(actions_clamped.detach().data.numpy().reshape(-1))\n",
    "        \n",
    "        memory.append((actions, reward, state, next_state, done))\n",
    "        \n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        total_steps += 1\n",
    "        \n",
    "        writer.add_scalar(\"dists/mean\", dists.loc[0], global_step=total_steps)\n",
    "        writer.add_scalar(\"dists/scale\", dists.scale[0], global_step=total_steps)\n",
    "\n",
    "        if done or (steps % max_steps == 0):\n",
    "            last_value = critic(t(next_state)).data.numpy()\n",
    "            learner.learn(memory, last_value, total_steps)\n",
    "            memory.clear()\n",
    "            \n",
    "        # env.render()\n",
    "    \n",
    "    if len(episode_rewards) % 10 == 0:\n",
    "        print(\"episode:\", len(episode_rewards), \", episode reward:\", total_reward)\n",
    "    writer.add_scalar(\"episode_reward\", total_reward, global_step=total_steps)\n",
    "    episode_rewards.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-397.219121377562"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(episode_rewards[-800:-700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2877,  0.0264, -0.1438,  0.5027,  1.0066, -0.1544,  0.4802, -0.1561,\n",
       "         0.2354, -0.0054,  0.3285,  0.1031,  0.7267, -0.0575, -0.0306,  0.7434,\n",
       "         0.4232, -0.1967,  0.2230, -0.1707,  0.0087, -0.2611,  0.7855,  0.3474,\n",
       "         0.8359,  0.1104, -0.2812, -0.2202,  0.3276,  0.1480, -0.3086, -0.1037],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = actor.model(t(state))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0014], grad_fn=<SoftplusBackward>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.stds_head(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â diario\n",
    "\n",
    "*\n",
    "* architecture review\n",
    "  all tanh, std out of the network, from 32 => 64 neuros\n",
    "* gradient clip\n",
    "\n",
    "scale going up always, changed lr (increased), nsteps (5 => 16), gamma (0.95 => 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3, 3),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(3, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.model(X)\n",
    "        mean = out[:, :1]\n",
    "        logstd = out[:, 1:]\n",
    "        \n",
    "        return mean, logstd.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09484435],\n",
       "       [ 0.66010272],\n",
       "       [ 0.28752064],\n",
       "       [ 1.33446952],\n",
       "       [ 0.74480515],\n",
       "       [-0.60730657],\n",
       "       [ 1.53686034],\n",
       "       [ 1.13118125],\n",
       "       [-0.60768608],\n",
       "       [-1.03437055]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_np = np.random.randint(1, 5, (10, 3))\n",
    "x_t = torch.from_numpy(x_np).float()\n",
    "\n",
    "network = Network()\n",
    "\n",
    "def sample_from_network(network, X):\n",
    "    mean, std = network(X)\n",
    "    return np.random.normal(0, 1, (len(X), 1))*std.data.numpy() + mean.data.numpy()\n",
    "    \n",
    "sample(network, x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
